{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import scipy.signal\n",
    "import os\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "def ffNetwork(x, action_dim, name = None):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        layerSizes = [64, 64]\n",
    "        z = x\n",
    "        for layer in layerSizes:\n",
    "            z = tf.contrib.layers.fully_connected(z, layer, activation_fn = tf.nn.tanh)\n",
    "        mean = tf.contrib.layers.fully_connected(z, action_dim, activation_fn = None)\n",
    "    return mean\n",
    "\n",
    "def valueNetwork(x, action_dim, name = None):\n",
    "    with tf.variable_scope(name):\n",
    "        layerSizes = [64, 64]\n",
    "        z = x\n",
    "        for layer in layerSizes:\n",
    "            z = tf.contrib.layers.fully_connected(z, layer, activation_fn = tf.nn.tanh)\n",
    "        z = tf.contrib.layers.fully_connected(z, 1, activation_fn = None)\n",
    "    return z\n",
    "\n",
    "class ExperienceBuffer():\n",
    "    \n",
    "    def __init__(self, observation_dim, action_dim, max_len = 1000):\n",
    "\n",
    "        #Taken from spinning it up - OpenAI\n",
    "        self.obs_list = np.zeros((max_len, observation_dim))\n",
    "        self.action_list = np.zeros((max_len, action_dim))\n",
    "        self.reward_list = np.zeros(max_len)\n",
    "        self.return_list = np.zeros(max_len)\n",
    "        self.adv_list = np.zeros(max_len)\n",
    "        self.logp_list = np.zeros(max_len)\n",
    "        self.val_list = np.zeros(max_len)\n",
    "        self.gamma, self.lam = .99, .97\n",
    "        self.point, self.max_len = 0, 1000\n",
    "        self.path_start_idx = 0\n",
    "       \n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        self.obs_list[self.point] = obs\n",
    "        self.action_list[self.point] = act\n",
    "        self.reward_list[self.point] = rew\n",
    "        self.val_list[self.point] = val\n",
    "        self.logp_list[self.point] = logp\n",
    "        self.point += 1\n",
    "\n",
    "    def _discount(self, x, discount):\n",
    "        '''\n",
    "        for i in reversed(range(discountList.shape[0]-1)):\n",
    "            discountList[i] = discount*discountList[i+1]\n",
    "        return discountList\n",
    "        '''\n",
    "        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "    def finish_traj(self, last_val=0):\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.point)\n",
    "        rews = np.append(self.reward_list[path_slice], last_val)\n",
    "        vals = np.append(self.val_list[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_list[path_slice] = self._discount(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.return_list[path_slice] = self._discount(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.point\n",
    "        return self.return_list[self.path_start_idx]\n",
    "\n",
    "    def get(self):\n",
    "        \n",
    "        self.point, self.path_start_idx = 0, 0\n",
    "        return [self.obs_list, self.action_list, self.logp_list, self.adv_list, self.logp_list]\n",
    "\n",
    "class PPO:\n",
    "    \n",
    "    def __init__(self, observation_dim, action_dim, batch_size, savePath):\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.savePath = savePath\n",
    "        \n",
    "        eps = 0.2\n",
    "        self.x = tf.placeholder(dtype = tf.float32, shape = observation_dim)\n",
    "        self.action = tf.placeholder(dtype = tf.float32, shape = (None, action_dim))\n",
    "        self.epsRewards = tf.placeholder(dtype = tf.float32, shape = None)        \n",
    "        self.adv = tf.placeholder(dtype = tf.float32, shape = None)\n",
    "        self.batch_size = batch_size\n",
    "        self.old_log_prob = tf.placeholder(dtype = tf.float32, shape = None)\n",
    "\n",
    "        meanCurrent = ffNetwork(self.x, action_dim, name = 'CurrentPolicyNetwork')\n",
    "\n",
    "        stdCurrent = tf.get_variable(name='log_std', initializer=-0.5*np.ones(action_dim, dtype=np.float32))\n",
    "        std = tf.exp(stdCurrent)\n",
    "\n",
    "        print(\"Shapes: {}, {}\".format(meanCurrent.shape, stdCurrent.shape))\n",
    "        #self.policyDist = tfp.distributions.MultivariateNormalDiag(meanCurrent, stdCurrent)\n",
    "        #self.policyEntropy = self.policyDist.entropy()\n",
    "        self.policyOut = meanCurrent + tf.random_normal(tf.shape(meanCurrent))*stdCurrent\n",
    "\n",
    "        print(\"Policy Sample Shape: {}\".format(self.policyOut.shape))\n",
    "        self.valueOut = valueNetwork(self.x, 1, name = 'ValueNetwork')\n",
    "        \n",
    "        bottomClip = (1-eps)\n",
    "        topClip = (1+eps)\n",
    "        min_adv = tf.where(self.adv > 0, topClip*self.adv, bottomClip*self.adv) \n",
    "        #Log prob in negative terms (log_prob = neg_log_prob)\n",
    "        self.current_log_prob = self.neg_log_prob(self.action, meanCurrent, stdCurrent)\n",
    "        self.policy_log_prob = self.neg_log_prob(self.policyOut, meanCurrent, stdCurrent)\n",
    "\n",
    "        policyRatio = tf.exp(self.current_log_prob - self.old_log_prob)\n",
    "        clipped_objective = tf.reduce_mean(tf.minimum(min_adv, policyRatio*self.adv))\n",
    "\n",
    "        self.valueObjective = (1/2)*tf.reduce_mean((self.valueOut - self.epsRewards)**2)\n",
    "        #self.combinedLoss = -clipped_objective + self.valueObjective\n",
    "        \n",
    "        policyParam = [v for v in tf.trainable_variables() if 'CurrentPolicyNetwork' in v.name]\n",
    "        valueParam = [v for v in tf.trainable_variables() if 'ValueNetwork' in v.name]   \n",
    "        self.trainPolicy = tf.train.AdamOptimizer(3e-4).minimize(-clipped_objective, var_list = policyParam) #Take the negativie objective to perform gradient ascent\n",
    "        self.trainValue = tf.train.AdamOptimizer(1e-3).minimize(self.valueObjective, var_list = valueParam)\n",
    "        #self.trainModel = tf.train.AdamOptimizer(1e-5).minimize(self.combinedLoss)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        print(\"Initialized Model\")\n",
    "\n",
    "    def predictPolicy(self, obs):\n",
    "        return self.sess.run([self.policyOut, self.policy_log_prob, self.valueOut], feed_dict = {self.x: obs})\n",
    "    \n",
    "    def predictValue(self, obs):\n",
    "        return self.sess.run(self.valueOut, feed_dict = {self.x: obs})\n",
    "   \n",
    "    def compute_entropy(self, probs):\n",
    "        return tf.reduce_sum(probs*tf.log(probs))\n",
    "\n",
    "    def neg_log_prob(self, x, mu, log_std):\n",
    "        #Returns the negative log pdf for a diagonal multivariate gaussian\n",
    "        pre_sum = 0.5 * (((x-mu)/(tf.exp(log_std)+EPS))**2 + 2*log_std + np.log(2*np.pi))\n",
    "        return tf.reduce_sum(pre_sum, axis=1)\n",
    "        #return (int(x.get_shape()[-1])/2)*tf.log(2*np.pi) + tf.reduce_sum(tf.log(std), axis = -1) + (0.5)*tf.reduce_sum(tf.square((x-mean)/std), axis = -1) #Axis = -1 to sum across normal dim\n",
    "\n",
    "    def computeAR(self, rewards, states, values, discount = 0.99, lmbda = 0.95, useGAE = True):\n",
    "       \n",
    "        #Computes advantage and return\n",
    "        #State shape - t_step x batch x dim\n",
    "        advList = np.zeros(rewards.shape)\n",
    "        deltas = rewards[:-1] + discount*values[1:] - values[:-1]\n",
    "        \n",
    "        advList = self._discount(deltas, lmbda*discount)\n",
    "        returnList = self._discount(rewards, discount)[:-1]\n",
    "\n",
    "        return advList,returnList\n",
    "    \n",
    "    def _discount(self, x, discount):\n",
    "        '''\n",
    "        for i in reversed(range(discountList.shape[0]-1)):\n",
    "            discountList[i] = discount*discountList[i+1]\n",
    "        return discountList\n",
    "        '''\n",
    "        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "        \n",
    "    def trainingStep(self, buffer_traj, gamma = 0.99, mini_batch = 64, epochs = 10):\n",
    "        \n",
    "        obs, actions, logprobs, adv, returnSet = buffer_traj\n",
    "\n",
    "        #Takes in an input of an episode trajectory\n",
    "        #adv, returnSet = self.computeAR(rewards, obs, values)\n",
    "        adv = (adv-np.mean(adv))/(np.std(adv)+1e-8) #Normalize advantage estimate - 1e-8 to prevent dividing by 0\n",
    "        adv = np.squeeze(adv)\n",
    "        #Returns a GAE estimate at every observation step\n",
    "\n",
    "        obs = np.squeeze(obs)\n",
    "        #Adv is a one dimensional list\n",
    "        \n",
    "        rdIdx = np.arange(obs.shape[0])\n",
    "        for _ in range(epochs):\n",
    "            cIdx = 0\n",
    "            endIdx = mini_batch\n",
    "        \n",
    "            while endIdx < obs.shape[0]:\n",
    "                batchIdx = rdIdx[cIdx: endIdx]\n",
    "                \n",
    "                self.sess.run(self.trainPolicy, feed_dict = {self.x: obs[batchIdx], self.adv: adv[batchIdx], self.action: actions[batchIdx], self.old_log_prob: logprobs[batchIdx]})\n",
    "                self.sess.run(self.trainValue, feed_dict = {self.x: obs[batchIdx], self.epsRewards: returnSet[batchIdx]})\n",
    "   \n",
    "                cIdx += mini_batch\n",
    "                endIdx += mini_batch\n",
    "            \n",
    "            batchIdx= rdIdx[cIdx:]\n",
    "            #batchIdx = np.arange(obs.shape[0]+1)\n",
    "            self.sess.run(self.trainPolicy, feed_dict = {self.x: obs[batchIdx], self.adv: adv[batchIdx], self.action: actions[batchIdx], self.old_log_prob: logprobs[batchIdx]})\n",
    "            self.sess.run(self.trainValue, feed_dict = {self.x: obs[batchIdx], self.epsRewards: returnSet[batchIdx]})\n",
    "            #np.random.shuffle(rdIdx)\n",
    "            \n",
    "    def getParam(self):\n",
    "        cParam = [v for v in tf.trainable_variables() if 'CurrentPolicyNetwork' in v.name]\n",
    "        cParam = sorted(cParam, key = lambda v: v.name)\n",
    "        return cParam\n",
    "\n",
    "    def save_variables(self):\n",
    "        if not os.path.isdir(self.savePath):\n",
    "            os.mkdir(self.savePath)\n",
    "        self.saver.save(self.sess, './'+self.savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
